{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of the snake parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to find the best parametrization for Cobra, i.e get the parameters that make the learninship as efficient as possible. First let import the files we need : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pygame\n",
    "import random\n",
    "\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from Env import Env\n",
    "from Viper import Viper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cobra :\n",
    "    def __init__(self, epsilon_decay = 0.995, learning_rate = 0.001):\n",
    "        self.memory  = deque(maxlen=2000)\n",
    "\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.model = self.create_model()\n",
    "        # \"hack\" implemented by DeepMind to improve convergence\n",
    "        self.target_model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(10, input_dim=8, activation=\"relu\"))\n",
    "        model.add(Dense(24, activation=\"relu\"))\n",
    "        model.add(Dense(24, activation=\"relu\"))\n",
    "        model.add(Dense(4, activation='linear'))\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def replay(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(new_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i]\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        # Exploration\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return random.randint(0,3)\n",
    "\n",
    "        return np.argmax(self.model.predict(state)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cobra_main(dqn_agent = Cobra(), nb_gens=10, max_action=500, show=True):\n",
    "    \"\"\"\n",
    "    Learns and play Snake\n",
    "    dqn_agent : Agent with preset\n",
    "    nb_gens = number of games for training\n",
    "    max_action = max number of action before next game\n",
    "    show : True if you want to play with no randomness after training\n",
    "    \"\"\"\n",
    "    env = Env(False)\n",
    "\n",
    "    # Nb games played with no random move\n",
    "    nb_games = 2\n",
    "\n",
    "    # Recording score\n",
    "    scores = [0] * nb_gens\n",
    "\n",
    "    for trial in range(nb_gens + nb_games):\n",
    "        print(\"Game : \", trial, \" | Epsilon is \", dqn_agent.epsilon)\n",
    "\n",
    "        # Does not show game until this end of the traning\n",
    "        if trial == nb_gens and show:\n",
    "            print(\"Traning Done\")\n",
    "            env.init_pygame()\n",
    "            pygame.event.get()\n",
    "            dqn_agent.epsilon = 0\n",
    "            print('--- Cobra is playing ---')\n",
    "\n",
    "        cur_state = np.array(env.reset()).reshape(1,8)\n",
    "        for step in range(max_action):\n",
    "            # Get the action from Cobra based on the current state\n",
    "            action = dqn_agent.act(cur_state)\n",
    "\n",
    "            # Applying move to environement to get a reward and a new state\n",
    "            new_state, reward, done = env.step(action)\n",
    "\n",
    "            # reward = reward if not done else -20\n",
    "            new_state = np.array(new_state).reshape(1,8)\n",
    "\n",
    "            # Remember the state, the move and the reward it got.\n",
    "            dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
    "\n",
    "            cur_state = new_state\n",
    "            # If has lost/won\n",
    "            if done:\n",
    "                dqn_agent.replay()\n",
    "                dqn_agent.target_train()\n",
    "                break\n",
    "        if trial < nb_gens:\n",
    "            scores[trial] = env.score\n",
    "    print(\"Done\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best parameters, you will train different Cobras using different values. The parameters we are going to modify are the followings :\n",
    "- self.epsilon_decay\n",
    "- self.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.25 0.5  0.75 1.  ]\n",
      "[0.0005 0.001  0.0015]\n"
     ]
    }
   ],
   "source": [
    "l_epsilon_rate = np.linspace(0,1,5)\n",
    "l_rate = np.linspace(0.0005,0.0015,3)\n",
    "print(l_epsilon_rate)\n",
    "print(l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.Cobra object at 0x7fd69b422690>, <__main__.Cobra object at 0x7fd69b9b4dd0>, <__main__.Cobra object at 0x7fd69b441750>]\n",
      "[<__main__.Cobra object at 0x7fd697b91510>, <__main__.Cobra object at 0x7fd697c78610>, <__main__.Cobra object at 0x7fd69aaea810>]\n",
      "[<__main__.Cobra object at 0x7fd69b3c5190>, <__main__.Cobra object at 0x7fd69b7a5e10>, <__main__.Cobra object at 0x7fd69793b790>]\n",
      "[<__main__.Cobra object at 0x7fd697c54350>, <__main__.Cobra object at 0x7fd69a40a250>, <__main__.Cobra object at 0x7fd697d5e650>]\n",
      "[<__main__.Cobra object at 0x7fd697c56d50>, <__main__.Cobra object at 0x7fd697c6f550>, <__main__.Cobra object at 0x7fd69b8ed590>]\n"
     ]
    }
   ],
   "source": [
    "matrix_cobra = []\n",
    "\n",
    "for eps in l_epsilon_rate :\n",
    "    matrix_rate = []\n",
    "    for rate in l_rate :\n",
    "        matrix_rate.append(Cobra(epsilon_decay = eps, learning_rate = rate))\n",
    "    matrix_cobra.append(matrix_rate)\n",
    "\n",
    "for line in matrix_cobra :\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon_decay :  1.0\n",
      "learning_rate :  0.0005\n",
      "Game :  0  | Epsilon is  1.0\n",
      "Game :  1  | Epsilon is  1.0\n",
      "Game :  2  | Epsilon is  1.0\n",
      "Game :  3  | Epsilon is  1.0\n",
      "Game :  4  | Epsilon is  1.0\n",
      "Game :  5  | Epsilon is  1.0\n",
      "Game :  6  | Epsilon is  1.0\n",
      "Game :  7  | Epsilon is  1.0\n",
      "Game :  8  | Epsilon is  1.0\n",
      "Game :  9  | Epsilon is  1.0\n",
      "Game :  10  | Epsilon is  1.0\n",
      "Game :  11  | Epsilon is  1.0\n",
      "Done\n",
      "The score : 0.1\n",
      "epsilon_decay :  1.0\n",
      "learning_rate :  0.001\n",
      "Game :  0  | Epsilon is  1.0\n",
      "Game :  1  | Epsilon is  1.0\n",
      "Game :  2  | Epsilon is  1.0\n",
      "Game :  3  | Epsilon is  1.0\n",
      "Game :  4  | Epsilon is  1.0\n",
      "Game :  5  | Epsilon is  1.0\n",
      "Game :  6  | Epsilon is  1.0\n",
      "Game :  7  | Epsilon is  1.0\n",
      "Game :  8  | Epsilon is  1.0\n",
      "Game :  9  | Epsilon is  1.0\n",
      "Game :  10  | Epsilon is  1.0\n",
      "Game :  11  | Epsilon is  1.0\n",
      "Done\n",
      "The score : 0.2\n",
      "epsilon_decay :  1.0\n",
      "learning_rate :  0.0015\n",
      "Game :  0  | Epsilon is  1.0\n",
      "Game :  1  | Epsilon is  1.0\n",
      "Game :  2  | Epsilon is  1.0\n",
      "Game :  3  | Epsilon is  1.0\n",
      "Game :  4  | Epsilon is  1.0\n",
      "Game :  5  | Epsilon is  1.0\n",
      "Game :  6  | Epsilon is  1.0\n",
      "Game :  7  | Epsilon is  1.0\n",
      "Game :  8  | Epsilon is  1.0\n",
      "Game :  9  | Epsilon is  1.0\n",
      "Game :  10  | Epsilon is  1.0\n",
      "Game :  11  | Epsilon is  1.0\n",
      "Done\n",
      "The score : 0.3\n",
      "epsilon_decay :  1.0\n",
      "learning_rate :  0.0005\n",
      "Game :  0  | Epsilon is  1.0\n",
      "Game :  1  | Epsilon is  1.0\n",
      "Game :  2  | Epsilon is  1.0\n",
      "Game :  3  | Epsilon is  1.0\n",
      "Game :  4  | Epsilon is  1.0\n",
      "Game :  5  | Epsilon is  1.0\n",
      "Game :  6  | Epsilon is  1.0\n",
      "Game :  7  | Epsilon is  1.0\n",
      "Game :  8  | Epsilon is  1.0\n",
      "Game :  9  | Epsilon is  1.0\n",
      "Game :  10  | Epsilon is  1.0\n",
      "Game :  11  | Epsilon is  1.0\n",
      "Done\n",
      "The score : 0.1\n",
      "epsilon_decay :  1.0\n",
      "learning_rate :  0.001\n",
      "Game :  0  | Epsilon is  1.0\n",
      "Game :  1  | Epsilon is  1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-0bb0e4917722>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"learning_rate : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcobra_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_gens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0ml_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The score :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-190-c02b7913502f>\u001b[0m in \u001b[0;36mcobra_main\u001b[0;34m(dqn_agent, nb_gens, max_action, show)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# If has lost/won\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-189-2de8c4c821b7>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mtarget_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mtarget_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1606\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     \"\"\"\n\u001b[1;32m   1804\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1806\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4212\u001b[0m         \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4213\u001b[0m         \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4214\u001b[0;31m         **self._flat_structure)\n\u001b[0m\u001b[1;32m   4215\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMapDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmap_dataset\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, name)\u001b[0m\n\u001b[1;32m   3023\u001b[0m         \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3024\u001b[0m         \u001b[0;34m\"use_inter_op_parallelism\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3025\u001b[0;31m         \"preserve_cardinality\", preserve_cardinality)\n\u001b[0m\u001b[1;32m   3026\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l_score = []\n",
    "for line in matrix_cobra :\n",
    "    for agent in matrix_rate :\n",
    "        \n",
    "        print(\"epsilon_decay : \", agent.epsilon_decay)\n",
    "        print(\"learning_rate : \", agent.learning_rate)\n",
    "        \n",
    "        score = np.mean(cobra_main(dqn_agent = agent, nb_gens = 10, max_action = 500, show = False))\n",
    "        l_score.append(score)\n",
    "        print('The score :', score)\n",
    "\n",
    "print(l_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark : the score is evaluated on two games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
